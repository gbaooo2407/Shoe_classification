import os
os.environ['HF_HOME'] = 'D:/huggingface'
os.environ['TRANSFORMERS_CACHE'] = 'D:/huggingface_cache'

from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

print("ƒêang t·∫£i m√¥ h√¨nh BLIP‚Äë2...")
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")

if torch.cuda.is_available():
    # S·ª≠ d·ª•ng 8-bit cho GPU (kh√¥ng c·∫ßn g·ªçi model.to(device))
    model = Blip2ForConditionalGeneration.from_pretrained(
        "Salesforce/blip2-opt-2.7b", 
        load_in_8bit=True, 
        device_map="auto"
    )
else:
    model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b")
    model.to(device)  # Ch·ªâ chuy·ªÉn model sang device khi kh√¥ng d√πng 8-bit

model.eval()
print("M√¥ h√¨nh ƒë√£ s·∫µn s√†ng.")

def generate_caption(image_path):
    """Generate a caption for an input image using BLIP‚Äë2"""
    try:
        # M·ªü v√† chuy·ªÉn ƒë·ªïi ·∫£nh
        image = Image.open(image_path).convert('RGB')
        inputs = processor(images=image, return_tensors="pt")
        dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        inputs = inputs.to(device, dtype)
        
        with torch.no_grad():
            out = model.generate(**inputs)
        
        if out is None:
            raise ValueError("Output from model.generate is None")
        
        caption = processor.decode(out[0], skip_special_tokens=True)
        return caption
    except Exception as e:
        print(f"L·ªói trong generate_caption: {str(e)}")
        return None
if __name__ == '__main__':
    image_path = r"D:\code DAP\·∫¢nh test\Shoe\istockphoto-1350560575-612x612.jpg"
    
    caption = generate_caption(image_path)
    print(f"üìù Caption: {caption}")

